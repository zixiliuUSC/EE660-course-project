{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from pathlib import Path\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = Path('review_train.txt')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 15:08:46,490 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-11-11 15:08:46,494 : INFO : collecting all words and their counts\n",
      "2019-11-11 15:08:46,498 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-11 15:08:48,236 : INFO : PROGRESS: at sentence #10000, processed 285506 words, keeping 6940 word types\n",
      "2019-11-11 15:08:48,635 : INFO : PROGRESS: at sentence #20000, processed 568771 words, keeping 9594 word types\n",
      "2019-11-11 15:08:48,948 : INFO : collected 11136 word types from a corpus of 788409 raw words and 27700 sentences\n",
      "2019-11-11 15:08:48,949 : INFO : Loading a fresh vocabulary\n",
      "2019-11-11 15:08:49,023 : INFO : effective_min_count=5 retains 3698 unique words (33% of original 11136, drops 7438)\n",
      "2019-11-11 15:08:49,024 : INFO : effective_min_count=5 leaves 777069 word corpus (98% of original 788409, drops 11340)\n",
      "2019-11-11 15:08:49,036 : INFO : deleting the raw counts dictionary of 11136 items\n",
      "2019-11-11 15:08:49,038 : INFO : sample=0.001 downsamples 70 most-common words\n",
      "2019-11-11 15:08:49,039 : INFO : downsampling leaves estimated 539036 word corpus (69.4% of prior 777069)\n",
      "2019-11-11 15:08:49,052 : INFO : estimated required memory for 3698 words and 50 dimensions: 3328200 bytes\n",
      "2019-11-11 15:08:49,053 : INFO : resetting layer weights\n",
      "2019-11-11 15:08:49,095 : INFO : training model with 3 workers on 3698 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-11 15:08:50,106 : INFO : EPOCH 1 - PROGRESS: at 72.13% examples, 386438 words/s, in_qsize 0, out_qsize 0\n",
      "2019-11-11 15:08:50,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 15:08:50,472 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 15:08:50,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 15:08:50,480 : INFO : EPOCH - 1 : training on 788409 raw words (539191 effective words) took 1.4s, 390835 effective words/s\n",
      "2019-11-11 15:08:51,663 : INFO : EPOCH 2 - PROGRESS: at 62.01% examples, 282924 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-11 15:08:52,058 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 15:08:52,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 15:08:52,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 15:08:52,064 : INFO : EPOCH - 2 : training on 788409 raw words (538930 effective words) took 1.6s, 340722 effective words/s\n",
      "2019-11-11 15:08:53,087 : INFO : EPOCH 3 - PROGRESS: at 64.51% examples, 340764 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-11 15:08:53,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 15:08:53,436 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 15:08:53,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 15:08:53,439 : INFO : EPOCH - 3 : training on 788409 raw words (539003 effective words) took 1.4s, 392687 effective words/s\n",
      "2019-11-11 15:08:54,479 : INFO : EPOCH 4 - PROGRESS: at 66.87% examples, 348119 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-11 15:08:54,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 15:08:54,801 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 15:08:54,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 15:08:54,807 : INFO : EPOCH - 4 : training on 788409 raw words (539026 effective words) took 1.4s, 394682 effective words/s\n",
      "2019-11-11 15:08:55,823 : INFO : EPOCH 5 - PROGRESS: at 65.68% examples, 349779 words/s, in_qsize 6, out_qsize 0\n",
      "2019-11-11 15:08:56,168 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 15:08:56,170 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 15:08:56,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 15:08:56,175 : INFO : EPOCH - 5 : training on 788409 raw words (539326 effective words) took 1.4s, 395009 effective words/s\n",
      "2019-11-11 15:08:56,175 : INFO : training on a 3942045 raw words (2695476 effective words) took 7.1s, 380711 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, size=50, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.0833205e-01 -1.1756685e+00  6.4095998e-01  6.6339827e-01\n",
      " -7.5053900e-01 -8.4961849e-01  9.2723423e-01  9.4702011e-01\n",
      "  1.9665561e+00  1.4826257e+00 -3.6097944e-01  3.6440468e-01\n",
      " -3.4359735e-02 -3.1991470e-01  6.9075519e-01 -8.3674842e-01\n",
      " -6.5302539e-01 -2.7462286e-01 -6.0682647e-02  1.7156234e-01\n",
      "  1.4226558e+00  3.4869423e+00 -5.1527399e-01 -1.4684041e+00\n",
      "  4.7869569e-01  4.3927872e-01  4.7020718e-01 -1.1643695e+00\n",
      " -1.1153636e+00 -4.5918792e-01 -5.2370137e-01 -1.3456216e+00\n",
      "  1.3700310e+00  4.4793916e-01  1.0821446e+00  1.0134779e-01\n",
      "  2.5098896e-01 -1.9094574e+00  9.6755546e-01 -6.3741595e-02\n",
      " -1.3932328e+00 -5.0843567e-01  8.8468164e-01  7.3833501e-01\n",
      "  1.3680323e+00 -2.3717621e-01 -8.0904150e-01  9.2776704e-01\n",
      "  6.7396644e-03  8.9600450e-01 -2.0379233e+00  1.2097522e+00\n",
      "  1.1523646e+00  4.8002941e-04  4.4422314e-02 -1.0450132e+00\n",
      " -1.6381912e+00 -1.8328540e+00  6.9814098e-01  1.1026750e+00\n",
      "  1.9387311e+00 -5.9344751e-01  2.7215949e-01 -9.2566407e-01\n",
      "  4.0499520e-02 -1.0663759e+00 -6.0652709e-01 -7.4136478e-01\n",
      " -4.8365763e-01 -9.6119481e-01 -9.6292508e-01 -1.9165113e+00\n",
      "  1.0888521e-01 -3.5451239e-01 -3.1521988e-01  1.0176599e+00\n",
      " -6.6702431e-01 -2.0262890e+00  7.0711774e-01  2.8970274e-01\n",
      "  1.2523376e+00  1.0009758e+00  1.9263407e+00 -5.1321828e-01\n",
      "  1.4027115e+00  2.4835828e-01 -1.6766832e+00  3.6794367e-01\n",
      "  3.9184007e-01 -4.9851054e-01  6.3824892e-01  3.6684480e-01\n",
      "  4.1722428e-02  7.1420916e-03 -1.8295906e-01 -3.7320116e-01\n",
      " -1.1497029e+00 -8.7008268e-01  1.0407850e+00  1.4311727e+00]\n"
     ]
    }
   ],
   "source": [
    "vec_kindle = model.wv['kindle']\n",
    "print(vec_kindle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy\n",
      "set\n",
      "up\n",
      "and\n",
      "simple\n",
      "to\n",
      "learn\n",
      "various\n",
      "functions\n",
      "including\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 15:09:27,005 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2019-11-11 15:09:27,006 : INFO : not storing attribute vectors_norm\n",
      "2019-11-11 15:09:27,007 : INFO : not storing attribute cum_table\n",
      "2019-11-11 15:09:27,033 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "#save a model\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-08 14:43:42,900 : INFO : loading Word2Vec object from word2vec.model\n",
      "2019-11-08 14:43:42,933 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2019-11-08 14:43:42,934 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-08 14:43:42,934 : INFO : loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "2019-11-08 14:43:42,935 : INFO : loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "2019-11-08 14:43:42,936 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-08 14:43:42,937 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "#load a model\n",
    "new_model = gensim.models.Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.0833205e-01 -1.1756685e+00  6.4095998e-01  6.6339827e-01\n",
      " -7.5053900e-01 -8.4961849e-01  9.2723423e-01  9.4702011e-01\n",
      "  1.9665561e+00  1.4826257e+00 -3.6097944e-01  3.6440468e-01\n",
      " -3.4359735e-02 -3.1991470e-01  6.9075519e-01 -8.3674842e-01\n",
      " -6.5302539e-01 -2.7462286e-01 -6.0682647e-02  1.7156234e-01\n",
      "  1.4226558e+00  3.4869423e+00 -5.1527399e-01 -1.4684041e+00\n",
      "  4.7869569e-01  4.3927872e-01  4.7020718e-01 -1.1643695e+00\n",
      " -1.1153636e+00 -4.5918792e-01 -5.2370137e-01 -1.3456216e+00\n",
      "  1.3700310e+00  4.4793916e-01  1.0821446e+00  1.0134779e-01\n",
      "  2.5098896e-01 -1.9094574e+00  9.6755546e-01 -6.3741595e-02\n",
      " -1.3932328e+00 -5.0843567e-01  8.8468164e-01  7.3833501e-01\n",
      "  1.3680323e+00 -2.3717621e-01 -8.0904150e-01  9.2776704e-01\n",
      "  6.7396644e-03  8.9600450e-01 -2.0379233e+00  1.2097522e+00\n",
      "  1.1523646e+00  4.8002941e-04  4.4422314e-02 -1.0450132e+00\n",
      " -1.6381912e+00 -1.8328540e+00  6.9814098e-01  1.1026750e+00\n",
      "  1.9387311e+00 -5.9344751e-01  2.7215949e-01 -9.2566407e-01\n",
      "  4.0499520e-02 -1.0663759e+00 -6.0652709e-01 -7.4136478e-01\n",
      " -4.8365763e-01 -9.6119481e-01 -9.6292508e-01 -1.9165113e+00\n",
      "  1.0888521e-01 -3.5451239e-01 -3.1521988e-01  1.0176599e+00\n",
      " -6.6702431e-01 -2.0262890e+00  7.0711774e-01  2.8970274e-01\n",
      "  1.2523376e+00  1.0009758e+00  1.9263407e+00 -5.1321828e-01\n",
      "  1.4027115e+00  2.4835828e-01 -1.6766832e+00  3.6794367e-01\n",
      "  3.9184007e-01 -4.9851054e-01  6.3824892e-01  3.6684480e-01\n",
      "  4.1722428e-02  7.1420916e-03 -1.8295906e-01 -3.7320116e-01\n",
      " -1.1497029e+00 -8.7008268e-01  1.0407850e+00  1.4311727e+00]\n"
     ]
    }
   ],
   "source": [
    "print(new_model.wv['kindle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27700,) (27699,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [27699, 27700]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-171f2714a618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# whole training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m train, val, y_train, y_val = train_test_split(x,\n\u001b[1;32m---> 14\u001b[1;33m                         y,test_size=0.2,random_state=10,stratify=y_tr)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mave_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2184\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [27699, 27700]"
     ]
    }
   ],
   "source": [
    "#read word2vec vector and generate average sentence vector\n",
    "#generate a doc vector by averaging the word2vec vector\n",
    "#use gensim.utils.simple_preprocess function\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "df_train = pd.read_csv('review_train.csv')\n",
    "df_train = df_train.dropna(axis=0,how='any')\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "x = df_train['review'].to_numpy()\n",
    "y = pd.read_csv('rate_train.csv')\n",
    "y = y['rate'].to_numpy()\n",
    "y = np.delete(y,12675,axis=0)# we don't have comment on idx = 12675\n",
    "\n",
    "# whole training set\n",
    "train, val, y_train, y_val = train_test_split(x,\n",
    "                        y,test_size=0.2,random_state=10,stratify=y_tr)\n",
    "\n",
    "ave_vec = np.zeros((train.shape[0],50),dtype='float')\n",
    "word_vectors = model.wv\n",
    "len(word_vectors.vocab)\n",
    "for k, seq in enumerate(train):\n",
    "    tokens = gensim.utils.simple_preprocess(seq)\n",
    "    l = 0.\n",
    "    for i in tokens:\n",
    "        if i in word_vectors.vocab: \n",
    "            ave_vec[k] += model.wv[i]\n",
    "            l += 1\n",
    "    ave_vec[k] /= l\n",
    "print(ave_vec.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 16:10:20,764 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-11-11 16:10:20,765 : INFO : collecting all words and their counts\n",
      "2019-11-11 16:10:20,765 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2019-11-11 16:10:20,766 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-11 16:10:20,900 : INFO : PROGRESS: at sentence #10000, processed 1485561 words, keeping 36 word types\n",
      "2019-11-11 16:10:21,033 : INFO : PROGRESS: at sentence #20000, processed 2990807 words, keeping 36 word types\n",
      "2019-11-11 16:10:21,064 : INFO : collected 36 word types from a corpus of 3329414 raw words and 22159 sentences\n",
      "2019-11-11 16:10:21,065 : INFO : Loading a fresh vocabulary\n",
      "2019-11-11 16:10:21,065 : INFO : effective_min_count=5 retains 32 unique words (88% of original 36, drops 4)\n",
      "2019-11-11 16:10:21,066 : INFO : effective_min_count=5 leaves 3329408 word corpus (99% of original 3329414, drops 6)\n",
      "2019-11-11 16:10:21,067 : INFO : deleting the raw counts dictionary of 36 items\n",
      "2019-11-11 16:10:21,067 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-11-11 16:10:21,068 : INFO : downsampling leaves estimated 561720 word corpus (16.9% of prior 3329408)\n",
      "2019-11-11 16:10:21,068 : INFO : estimated required memory for 32 words and 50 dimensions: 28800 bytes\n",
      "2019-11-11 16:10:21,069 : INFO : resetting layer weights\n",
      "2019-11-11 16:10:21,071 : INFO : training model with 3 workers on 32 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-11 16:10:21,494 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:10:21,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:10:21,496 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:10:21,497 : INFO : EPOCH - 1 : training on 3329414 raw words (561161 effective words) took 0.4s, 1329164 effective words/s\n",
      "2019-11-11 16:10:21,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:10:21,914 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:10:21,915 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:10:21,916 : INFO : EPOCH - 2 : training on 3329414 raw words (562169 effective words) took 0.4s, 1349701 effective words/s\n",
      "2019-11-11 16:10:22,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:10:22,356 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:10:22,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:10:22,357 : INFO : EPOCH - 3 : training on 3329414 raw words (561912 effective words) took 0.4s, 1278643 effective words/s\n",
      "2019-11-11 16:10:22,789 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:10:22,791 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:10:22,792 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:10:22,793 : INFO : EPOCH - 4 : training on 3329414 raw words (560892 effective words) took 0.4s, 1296010 effective words/s\n",
      "2019-11-11 16:10:23,214 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:10:23,216 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:10:23,216 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:10:23,217 : INFO : EPOCH - 5 : training on 3329414 raw words (561737 effective words) took 0.4s, 1334556 effective words/s\n",
      "2019-11-11 16:10:23,218 : INFO : training on a 16647070 raw words (2807871 effective words) took 2.1s, 1308505 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train = pd.read_csv('review_train.csv')\n",
    "df_train = df_train.dropna(axis=0,how='any')\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "x = df_train['review'].to_numpy()\n",
    "y = pd.read_csv('rate_train.csv')\n",
    "y = y['rate'].to_numpy()\n",
    "y = np.delete(y,12675,axis=0)# we don't have comment on idx = 12675\n",
    "# whole training set\n",
    "#train, val, y_train, y_val = train_test_split(x,\n",
    "#                        y,test_size=0.2,random_state=10,stratify=y)\n",
    "model = gensim.models.Word2Vec(sentences=train, size=50, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outfile = Path('../input/train.txt')\n",
    "with outfile.open('w',encoding='utf-8') as w:\n",
    "    for k in range(train.shape[0]):\n",
    "        w.write(train[k]+'\\n')\n",
    "pd.DataFrame({'review':train}).to_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "outfile = Path('../input/val.txt')\n",
    "with outfile.open('w',encoding='utf-8') as w:\n",
    "    for k in range(val.shape[0]):\n",
    "        w.write(val[k]+'\\n')\n",
    "pd.DataFrame({'review':val}).to_csv('../input/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'rate':y_train}).to_csv('../input/y_train.csv')\n",
    "pd.DataFrame({'rate':y_val}).to_csv('../input/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 16:23:05,009 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-11-11 16:23:05,009 : INFO : collecting all words and their counts\n",
      "2019-11-11 16:23:05,010 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2019-11-11 16:23:05,011 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-11 16:23:05,145 : INFO : PROGRESS: at sentence #10000, processed 1485561 words, keeping 36 word types\n",
      "2019-11-11 16:23:05,273 : INFO : PROGRESS: at sentence #20000, processed 2990807 words, keeping 36 word types\n",
      "2019-11-11 16:23:05,302 : INFO : collected 36 word types from a corpus of 3329414 raw words and 22159 sentences\n",
      "2019-11-11 16:23:05,303 : INFO : Loading a fresh vocabulary\n",
      "2019-11-11 16:23:05,304 : INFO : effective_min_count=5 retains 32 unique words (88% of original 36, drops 4)\n",
      "2019-11-11 16:23:05,304 : INFO : effective_min_count=5 leaves 3329408 word corpus (99% of original 3329414, drops 6)\n",
      "2019-11-11 16:23:05,305 : INFO : deleting the raw counts dictionary of 36 items\n",
      "2019-11-11 16:23:05,306 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-11-11 16:23:05,306 : INFO : downsampling leaves estimated 561720 word corpus (16.9% of prior 3329408)\n",
      "2019-11-11 16:23:05,308 : INFO : estimated required memory for 32 words and 50 dimensions: 28800 bytes\n",
      "2019-11-11 16:23:05,309 : INFO : resetting layer weights\n",
      "2019-11-11 16:23:05,310 : INFO : training model with 3 workers on 32 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-11 16:23:05,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:23:05,730 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:23:05,731 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:23:05,732 : INFO : EPOCH - 1 : training on 3329414 raw words (561550 effective words) took 0.4s, 1342994 effective words/s\n",
      "2019-11-11 16:23:06,154 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:23:06,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:23:06,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:23:06,156 : INFO : EPOCH - 2 : training on 3329414 raw words (561062 effective words) took 0.4s, 1332366 effective words/s\n",
      "2019-11-11 16:23:06,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:23:06,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:23:06,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:23:06,620 : INFO : EPOCH - 3 : training on 3329414 raw words (561523 effective words) took 0.5s, 1218013 effective words/s\n",
      "2019-11-11 16:23:07,037 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:23:07,038 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:23:07,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:23:07,039 : INFO : EPOCH - 4 : training on 3329414 raw words (561398 effective words) took 0.4s, 1347328 effective words/s\n",
      "2019-11-11 16:23:07,460 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:23:07,461 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:23:07,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:23:07,462 : INFO : EPOCH - 5 : training on 3329414 raw words (562495 effective words) took 0.4s, 1340741 effective words/s\n",
      "2019-11-11 16:23:07,462 : INFO : training on a 16647070 raw words (2808028 effective words) took 2.2s, 1305058 effective words/s\n",
      "2019-11-11 16:23:07,463 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2019-11-11 16:23:07,464 : INFO : not storing attribute vectors_norm\n",
      "2019-11-11 16:23:07,465 : INFO : not storing attribute cum_table\n",
      "2019-11-11 16:23:07,469 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences=train, size=50, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 16:24:03,319 : INFO : saving Word2Vec object under ../embedding/word2vec.model, separately None\n",
      "2019-11-11 16:24:03,320 : INFO : not storing attribute vectors_norm\n",
      "2019-11-11 16:24:03,321 : INFO : not storing attribute cum_table\n",
      "2019-11-11 16:24:03,323 : INFO : saved ../embedding/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save('../embedding/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 16:38:45,967 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-11-11 16:38:45,968 : INFO : collecting all words and their counts\n",
      "2019-11-11 16:38:45,969 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-11 16:38:46,361 : INFO : PROGRESS: at sentence #10000, processed 280389 words, keeping 6861 word types\n",
      "2019-11-11 16:38:46,757 : INFO : PROGRESS: at sentence #20000, processed 564365 words, keeping 9431 word types\n",
      "2019-11-11 16:38:46,846 : INFO : collected 9929 word types from a corpus of 628177 raw words and 22159 sentences\n",
      "2019-11-11 16:38:46,847 : INFO : Loading a fresh vocabulary\n",
      "2019-11-11 16:38:46,854 : INFO : effective_min_count=5 retains 3319 unique words (33% of original 9929, drops 6610)\n",
      "2019-11-11 16:38:46,854 : INFO : effective_min_count=5 leaves 617952 word corpus (98% of original 628177, drops 10225)\n",
      "2019-11-11 16:38:46,866 : INFO : deleting the raw counts dictionary of 9929 items\n",
      "2019-11-11 16:38:46,867 : INFO : sample=0.001 downsamples 71 most-common words\n",
      "2019-11-11 16:38:46,868 : INFO : downsampling leaves estimated 427707 word corpus (69.2% of prior 617952)\n",
      "2019-11-11 16:38:46,876 : INFO : estimated required memory for 3319 words and 50 dimensions: 2987100 bytes\n",
      "2019-11-11 16:38:46,877 : INFO : resetting layer weights\n",
      "2019-11-11 16:38:46,915 : INFO : training model with 3 workers on 3319 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-11 16:38:47,964 : INFO : EPOCH 1 - PROGRESS: at 81.45% examples, 331060 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-11 16:38:48,052 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:38:48,055 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:38:48,059 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:38:48,060 : INFO : EPOCH - 1 : training on 628177 raw words (427974 effective words) took 1.1s, 374687 effective words/s\n",
      "2019-11-11 16:38:49,064 : INFO : EPOCH 2 - PROGRESS: at 81.45% examples, 345962 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-11 16:38:49,150 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:38:49,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:38:49,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:38:49,156 : INFO : EPOCH - 2 : training on 628177 raw words (427547 effective words) took 1.1s, 390824 effective words/s\n",
      "2019-11-11 16:38:50,164 : INFO : EPOCH 3 - PROGRESS: at 83.03% examples, 351678 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-11 16:38:50,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:38:50,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:38:50,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:38:50,247 : INFO : EPOCH - 3 : training on 628177 raw words (427870 effective words) took 1.1s, 393419 effective words/s\n",
      "2019-11-11 16:38:51,257 : INFO : EPOCH 4 - PROGRESS: at 84.77% examples, 356928 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-11 16:38:51,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:38:51,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:38:51,323 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:38:51,323 : INFO : EPOCH - 4 : training on 628177 raw words (427630 effective words) took 1.1s, 398271 effective words/s\n",
      "2019-11-11 16:38:52,334 : INFO : EPOCH 5 - PROGRESS: at 83.03% examples, 349947 words/s, in_qsize 4, out_qsize 0\n",
      "2019-11-11 16:38:52,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-11 16:38:52,429 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-11 16:38:52,430 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-11 16:38:52,431 : INFO : EPOCH - 5 : training on 628177 raw words (427722 effective words) took 1.1s, 387003 effective words/s\n",
      "2019-11-11 16:38:52,432 : INFO : training on a 3140885 raw words (2138743 effective words) took 5.5s, 387756 effective words/s\n",
      "2019-11-11 16:38:52,433 : INFO : saving Word2Vec object under ../embedding/word2vec.model, separately None\n",
      "2019-11-11 16:38:52,433 : INFO : not storing attribute vectors_norm\n",
      "2019-11-11 16:38:52,434 : INFO : not storing attribute cum_table\n",
      "2019-11-11 16:38:52,453 : INFO : saved ../embedding/word2vec.model\n",
      "2019-11-11 16:38:52,455 : INFO : loading Word2Vec object from ../embedding/word2vec.model\n",
      "2019-11-11 16:38:52,476 : INFO : loading wv recursively from ../embedding/word2vec.model.wv.* with mmap=None\n",
      "2019-11-11 16:38:52,477 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-11 16:38:52,478 : INFO : loading vocabulary recursively from ../embedding/word2vec.model.vocabulary.* with mmap=None\n",
      "2019-11-11 16:38:52,478 : INFO : loading trainables recursively from ../embedding/word2vec.model.trainables.* with mmap=None\n",
      "2019-11-11 16:38:52,479 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-11 16:38:52,479 : INFO : loaded ../embedding/word2vec.model\n",
      "C:\\Users\\Ou Zixi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22159, 50)\n"
     ]
    }
   ],
   "source": [
    "#generate a doc vector by averaging the word2vec vector\n",
    "#use gensim.utils.simple_preprocess function\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "df_train = pd.read_csv('../input/train.csv')\n",
    "train = df_train['review'].to_numpy()\n",
    "df_y_train = pd.read_csv('../input/y_train.csv')\n",
    "y_train = df_y_train['rate'].to_numpy()\n",
    "df_val = pd.read_csv('../input/val.csv')\n",
    "val = df_val['review'].to_numpy()\n",
    "df_y_val = pd.read_csv('../input/y_val.csv')\n",
    "y_val = df_y_val['rate'].to_numpy()\n",
    "ave_vec = np.zeros((train.shape[0],50),dtype='float')\n",
    "\n",
    "# train word2vec model\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from pathlib import Path\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = Path('../input/train.txt')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "import gensim.models\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, size=50, iter=5)\n",
    "model.save('../embedding/word2vec.model')\n",
    "model = gensim.models.Word2Vec.load('../embedding/word2vec.model')\n",
    "\n",
    "word_vectors = model.wv\n",
    "len(word_vectors.vocab)\n",
    "for k, seq in enumerate(train):\n",
    "    tokens = gensim.utils.simple_preprocess(seq)\n",
    "    l = 0.\n",
    "    for i in tokens:\n",
    "        if i in word_vectors.vocab: \n",
    "            ave_vec[k] += model.wv[i]\n",
    "            l += 1\n",
    "    ave_vec[k] /= l\n",
    "print(ave_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06212315,  0.27718165,  0.22014132,  0.01131697, -0.00367662,\n",
       "        0.22127536,  0.05665175, -0.0793856 ,  0.39512282,  0.0837888 ,\n",
       "       -0.05765808, -0.07202009, -0.81537582,  0.2933987 ,  0.39674547,\n",
       "        0.21490826, -0.16225971,  0.15515523,  0.18632433,  0.40472801,\n",
       "        0.18798339,  0.4906395 ,  0.12894624,  0.23313047, -0.0367707 ,\n",
       "        0.03583179,  0.0818591 , -0.16821547, -0.18210006, -0.23767604,\n",
       "       -0.42823224, -0.40969629, -0.02527377,  0.54764745,  0.26713975,\n",
       "        0.33606754, -0.14908146,  0.59201479, -0.13122277,  0.00610968,\n",
       "       -0.23669558,  0.12105098, -0.2502157 , -0.23825294,  0.40760303,\n",
       "       -0.23043296, -0.17254531, -0.06145687, -0.49371246,  0.00478465])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "\n",
    "lee_train_file = Path('review_train.txt')\n",
    "lee_test_file = Path('review_test.txt')\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))\n",
    "\n",
    "\n",
    "model_doc2vec = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "model_doc2vec.build_vocab(train_corpus)\n",
    "\n",
    "vector = model_doc2vec.infer_vector(['i', 'dont', 'like', 'kindle'])\n",
    "print(vector)\n",
    "\n",
    "# use a tokenizer to preprocess the text so that we can get a list to generate a document vector\n",
    "text = gensim.utils.simple_preprocess('the tablet is good for games like solitaire or other app games and of urse for books but for internet or other applications it would not be the top choice good buy for the money')\n",
    "print(model_doc2vec.infer_vector(text))\n",
    "\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model_doc2vec.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model_doc2vec.docvecs.most_similar([inferred_vector], topn=len(model_doc2vec.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)\n",
    "\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "\n",
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model_doc2vec.infer_vector(test_corpus[doc_id])\n",
    "sims = model_doc2vec.docvecs.most_similar([inferred_vector], topn=len(model_doc2vec.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model_doc2vec)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model_doc2vec.save('doc2vec.model')\n",
    "\n",
    "\n",
    "#read doc2vec vector\n",
    "#generate a doc vector by averaging the word2vec vector\n",
    "#use gensim.utils.simple_preprocess function\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "df_train = pd.read_csv('review_train.csv')\n",
    "df_train = df_train.dropna(axis=0,how='any')\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "train = df_train['review'].to_numpy()\n",
    "ave_vec = np.zeros((train.shape[0],50),dtype='float')\n",
    "model1 = Doc2Vec.load('doc2vec.model')\n",
    "for k, seq in enumerate(train):\n",
    "    tokens = gensim.utils.simple_preprocess(seq)\n",
    "    ave_vec[k] = model1.infer_vector(tokens)\n",
    "\n",
    "ave_vec.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
